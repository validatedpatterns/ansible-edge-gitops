---
# Source: portworx/templates/storageclass/rbac/serviceAccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: portworx-sc-sa
  namespace: portworx
  annotations:
    argocd.argoproj.io/sync-hook: "PreSync"
    argocd.argoproj.io/sync-wave: "-10"
---
# Source: portworx/templates/storageclass/portworx-rwx.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "4"
  name: px-csi-db-shared
parameters:
  io_profile: db_remote
  repl: "3"
  sharedv4: "true"
  sharedv4_svc_type: "ClusterIP"
provisioner: pxd.portworx.com
reclaimPolicy: Delete
volumeBindingMode: Immediate
allowVolumeExpansion: true
---
# Source: portworx/templates/storageclass/rbac/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    argocd.argoproj.io/hook: PreSync
    argocd.argoproj.io/sync-wave: "-15"
  name: portworx-sc-clusterrole
rules:
- apiGroups: ["*"]
  resources: ['pods','storageclusters']
  verbs: ['get','list']
---
# Source: portworx/templates/storageclass/rbac/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: portworx-sc-clusterrolebinding
  annotations:
    argocd.argoproj.io/hook: PreSync
    argocd.argoproj.io/sync-wave: "-15"
subjects:
- kind: ServiceAccount
  name: portworx-sc-sa
  namespace: portworx
  apiGroup: ""
roleRef:
  kind: ClusterRole
  name: portworx-sc-clusterrole
  apiGroup: rbac.authorization.k8s.io
---
# Source: portworx/templates/storageclass/rbac/role-ns.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  annotations:
    argocd.argoproj.io/hook: PreSync
    argocd.argoproj.io/sync-wave: "-15"
  namespace: portworx 
  name: portworx-sc-ns-role
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["*"]
---
# Source: portworx/templates/storageclass/rbac/rolebinding-ns.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: portworx-sc-ns-rolebinding
  annotations:
    argocd.argoproj.io/hook: PreSync
    argocd.argoproj.io/sync-wave: "-15"
subjects:
- kind: ServiceAccount
  name: portworx-sc-sa
  namespace: portworx
  apiGroup: ""
roleRef:
  kind: Role
  name: portworx-sc-ns-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: portworx/templates/storageclass/wait-for-pxe.yaml
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "3"
  name: job-wait-for-portworx
  namespace: portworx
spec:
  template:
    spec:
      containers:
      - image: quay.io/hybridcloudpatterns/utility-container
        command:
        - /bin/bash
        - -x
        - -c
        - |
          stc_status=$(oc get stc -n portworx | grep -E "Online|Running" | wc -l)
          until [ "$stc_status" -eq "1" ];
          do
            echo "Portworx storagecluster not yet online"
            sleep 10
            stc_status=$(oc get stc -n portworx | grep -E "Online|Running" | wc -l)
          done
          echo "Portworx storagecluster online, waiting for all containers to start"
          num_px_pods=$(oc get pod -l name=portworx -n portworx --no-headers | wc -l)
          while [ 1 ];
          do
            num_px_pods_ready=$(oc get pod -l name=portworx -n portworx |grep -P '\s+([1-9]+[\d]*)\/\1\s+' | wc -l)
            if [ "$num_px_pods_ready" -eq "$num_px_pods" ]; then
              echo "Portworx is ready, $num_px_pods_ready of $num_px_pods pods running 2/2"
              exit 0
            fi
            echo "Portworx is not yet ready, $num_px_pods_ready of $num_px_pods pods running 2/2"
            sleep 15
          done
        name: wait-for-portworx-ready
      dnsPolicy: ClusterFirst
      restartPolicy: Never
      serviceAccount: portworx-sc-sa
      serviceAccountName: portworx-sc-sa
      terminationGracePeriodSeconds: 600
---
# Source: portworx/templates/portworx-storagecluster.yaml
apiVersion: core.libopenstorage.org/v1
kind: StorageCluster
metadata:
  name: px-cluster-region
  namespace: portworx
  annotations:
    argocd.argoproj.io/sync-wave: "3"
    portworx.io/is-openshift: "true"
    portworx.com/install-source: helm-rhmcgo
    portworx.com/helm-vars: chart="portworx-0.0.1",cloudProvider="map[storageClass:default-rwo]" ,clusterGroup="map[applications:map[acm:map[ignoreDifferences:[map[group:internal.open-cluster-management.io jsonPointers:[/spec/loggingCA] kind:ManagedClusterInfo]] name:acm namespace:open-cluster-management path:common/acm project:datacenter] pipe:map[extraValueFiles:[/values/{{ .Values.global.clusterVersion }}/{{ .Values.global.clusterPlatform }}.yaml] name:pipelines namespace:application-ci path:charts/datacenter/pipelines project:datacenter]] imperative:map[jobs:[map[name:test playbook:ansible/test.yml timeout:234]] namespace:imperative] isHubCluster:true managedClusterGroups:[map[acmlabels:[map[name:clusterGroup value:acm-region]] helmOverrides:[map[name:clusterGroup.isHubCluster value:false]] name:acm-edge targetRevision:main] map[acmlabels:[map[name:clusterGroup value:region]] clusterPools:map[exampleAWSPool:map[baseDomain:blueprints.rhecoeng.com controlPlane:map[count:1 platform:map[aws:map[type:m5.xlarge]]] name:aws-ap openshiftVersion:4.10.18 platform:map[aws:map[region:ap-southeast-2]] size:3 workers:map[count:0]] exampleAzurePool:map[baseDomain:blueprints.rhecoeng.com clusters:[Two three] name:azure-us openshiftVersion:4.10.18 platform:map[azure:map[baseDomainResourceGroupName:dojo-dns-zones region:eastus]]]] helmOverrides:[map[name:clusterGroup.isHubCluster value:false]] name:acm-provision-edge targetRevision:main] map[helmOverrides:[map[name:clusterGroup.isHubCluster value:false]] hostedArgoSites:[map[domain:perth1.beekhof.net name:perth] map[domain:syd.beekhof.net name:sydney]] name:argo-edge]] name:example namespaces:[map[open-cluster-management:map[annotations:map[openshift.io/cluster-monitoring:true owner:namespace owner] labels:map[kubernetes.io/os:linux openshift.io/node-selector:]]] map[application-ci:map[operatorGroup:true targetNamespaces:[application-ci other-namespace]]] map[exclude-targetns:map[operatorGroup:true targetNamespaces:<nil>]] include-ci exclude-og map[totally-exclude-og:map[operatorGroup:false]] map[include-default-og:map[operatorGroup:true]]] operatorgroupExcludes:[exclude-og] projects:[datacenter] sharedValueFiles:[/values/{{ .Values.global.clusterPlatform }}.yaml /values/{{ .Values.global.clusterVersion }}.yaml] subscriptions:map[acm:map[channel:release-2.4 csv:advanced-cluster-management.v2.4.1 name:advanced-cluster-management namespace:open-cluster-management] odh:map[csv:opendatahub-operator.v1.1.0 disabled:true name:opendatahub-operator source:community-operators] pipelines:map[csv:redhat-openshift-pipelines.v1.5.2 name:openshift-pipelines-operator-rh]]]" ,csi="true" ,deleteStrategy="UninstallAndWipe" ,envVars="none" ,global="map[clusterDomain:region.example.com clusterPlatform:aws clusterVersion:4.12 hub:map[provider:aws storageClassName:gp2] hubClusterDomain:apps.hub.example.com localClusterDomain:apps.region.example.com multiClusterTarget:all namespace:pattern-namespace options:map[installPlanApproval:Automatic syncPolicy:Automatic useCSV:false] pattern:mypattern repoURL:https://github.com/pattern-clone/mypattern]" ,internalKVDB="true" ,main="map[clusterGroupName:hub git:map[repoURL:https://github.com/pattern-clone/mypattern revision:main]]" ,namespace="portworx" ,network="map[dataInterface:none managementInterface:none]" ,pxnamespace="portworx" ,repo="map[dr:docker.io/portworx enterprise:docker.io/portworx]" ,secretType="k8s" ,secrets="map[AWSsecretName:aws-creds AWSsecretNamespace:kube-system]" ,storage="map[drives:type=gp2,size=20 journalDevice:<nil> kvdbDrives:type=gp2,size=150 maxStorageNodesPerZone:1 usedrivesAndPartitions:false usefileSystemDrive:false]" ,versions="map[autoPilot:1.3.7 enterprise:2.13.4 ociMon:2.13.4 stork:23.4.0]" 
spec:
  deleteStrategy:
    type: UninstallAndWipe
  env:
    # TODO: Change this hardcoded image path to an ECR registry path with px-enterprise image (PWX-27961)
    - name: PX_IMAGE
      value: docker.io/portworx/px-enterprise:2.13.0
    - name: PX_NAMESPACE
      value: portworx
  image: "portworx/oci-monitor:2.13.4"
  imagePullPolicy: Always
  kvdb:
    internal: true
  cloudStorage:
    deviceSpecs:
    - type=gp2,size=20
    journalDeviceSpec: auto
    kvdbDeviceSpec: type=gp2,size=150
    maxStorageNodesPerZone: 1
  secretsProvider: k8s
  stork:
    enabled: true
    args:
      webhook-controller: "true"
    image: "openstorage/stork:23.4.0"
  autopilot:
    enabled: true
    image: "portworx/autopilot:1.3.7"
  csi:
    enabled: true
