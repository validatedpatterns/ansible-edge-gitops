---
# Source: portworx/templates/aws-prereqs/rbac/serviceAccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: portworx-prereq-sa
  namespace: portworx
  annotations:
    argocd.argoproj.io/sync-hook: "PreSync"
    argocd.argoproj.io/sync-wave: "-10"
---
# Source: portworx/templates/storageclass/rbac/serviceAccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: portworx-sc-sa
  namespace: portworx
  annotations:
    argocd.argoproj.io/sync-hook: "PreSync"
    argocd.argoproj.io/sync-wave: "-10"
---
# Source: portworx/templates/aws-prereqs/px-aws-prereqs-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "1"
  name: px-aws-prereqs-configmap
  namespace: portworx
data:
  px_aws_pre-reqs.sh: |	
    #!/bin/bash

    export AWS_CONFIG_FILE=/pattern-home/credentials/.aws/config
    export AWS_SHARED_CREDENTIALS_FILE=/pattern-home/credentials/.aws/credentials

    mkdir /pattern-home/credentials/.aws

    # Ensure we can access the secret for AWS credentials
    oc get secret aws-creds -n kube-system -o json
    if [ $? != 0 ];
    then
      echo "Cannot access aws-creds secret in kube-system namespace, please check rbac for portworx-prereq-sa serviceAccount."
    else
      echo "Found AWS credential secret, building credentials file."
      AWS_ACCESS_KEY=$(curl -sSk -H "Authorization: Bearer $(cat /run/secrets/kubernetes.io/serviceaccount/token)" https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_PORT_443_TCP_PORT/api/v1/namespaces/kube-system/secrets/aws-creds | jq -r '.data.aws_access_key_id' | base64 -d)
      AWS_SECRET_KEY=$(curl -sSk -H "Authorization: Bearer $(cat /run/secrets/kubernetes.io/serviceaccount/token)" https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_PORT_443_TCP_PORT/api/v1/namespaces/kube-system/secrets/aws-creds | jq -r '.data.aws_secret_access_key' | base64 -d)
      NODE_REG=$(oc get node -l 'node-role.kubernetes.io/worker=' -o=jsonpath='{.items[0].metadata.name}')
      AWS_REGION=$(oc get node $NODE_REG -o jsonpath='{.metadata.labels.topology\.kubernetes\.io/region}')
    cat << EOF >> /pattern-home/credentials/.aws/credentials
    [default]
    aws_access_key_id = $AWS_ACCESS_KEY
    aws_secret_access_key = $AWS_SECRET_KEY
    EOF

    cat << EOF >> /pattern-home/credentials/.aws/config
    [default]
    region = $AWS_REGION
    output = json
    EOF

    fi


    # Ensure we have AWS credentials to use
    echo "Checking for AWS credentials."
    aws sts get-caller-identity

    if [ $? != 0 ];
    then
      echo "AWS credentials not found, exiting."
      exit 1
    else
      echo "Found AWS credentials."
    fi

    # Ensure oc is functional and we can get node info from OCP cluster via serviceAccount
    oc get node
    if [ $? != 0 ];
    then
      echo "Cannot list nodes, please check rbac for portworx-prereq-sa serviceAccount."
      exit 1
    else
      echo "Configuring for Portworx/Red Hat Multicloud GitOps Pattern."
    fi
    
    # Ensure there are a minimum of three worker nodes in the cluster
    NUM_WORKERS=$(oc get node -l 'node-role.kubernetes.io/worker=' -o name | wc -l)
    if [ $NUM_WORKERS -lt 3 ];
    then
      echo "Only $NUM_WORKERS worker nodes detected - minimum required is three."
      exit 1
    else
      echo "Found $NUM_WORKERS worker nodes - configuring..."
    fi
    
    # Get the list of worker nodes in the cluster
    WORKERS=$(oc get node -l 'node-role.kubernetes.io/worker=' -o name)

    # Modify the SG created by OpenShift Installer to allow ports necessary for Portworx
    for NODE in $WORKERS; do
      unset AWS_INSTANCE
      unset AWS_REGION
      unset AWS_SG
      AWS_INSTANCE=$(oc get $NODE -o jsonpath='{.spec.providerID}' | sed 's|.*/||')
      AWS_REGION=$(oc get $NODE -o jsonpath='{.metadata.labels.topology\.kubernetes\.io/region}')
      AWS_SG=$(aws ec2 describe-instances --instance-id $AWS_INSTANCE --region=$AWS_REGION --query "Reservations[].Instances[].SecurityGroups[].GroupId[]" --output text)
      for GROUP in $AWS_SG; do
        OCP_SG=$(aws ec2 describe-security-groups --group-ids $GROUP --query SecurityGroups[*].Description | grep -w "Created By OpenShift Installer" | wc -l)
        if [ $OCP_SG -eq 1 ];
        then
          echo "Found OpenShift created security group assigned to $AWS_INSTANCE, opening Portworx ports."

          # Check to see if security group already allows TCP 17001-17022
          RULE_EXISTS_SG=$(aws ec2 describe-security-groups --region $AWS_REGION --filters Name=ip-permission.from-port,Values=17001 Name=ip-permission.to-port,Values=17022 Name=ip-permission.group-id,Values=$AWS_SG  --output text)
          if [ -z "$RULE_EXISTS_SG" ]
          then
            unset RULE_EXISTS_SG
            echo "Adding rule for TCP 17001-17022"
            aws ec2 authorize-security-group-ingress --group-id $GROUP --protocol tcp --port 17001-17022 --region $AWS_REGION --source-group $GROUP
          else
            unset RULE_EXISTS_SG
            echo "Ports TCP 17001-17022 already allowed in security group $AWS_SG for $AWS_INSTANCE."
          fi

           # Check to see if security group already allows TCP 20048
          RULE_EXISTS_SG=$(aws ec2 describe-security-groups --region $AWS_REGION --filters Name=ip-permission.from-port,Values=20048 Name=ip-permission.to-port,Values=20048 Name=ip-permission.group-id,Values=$AWS_SG  --output text)
          if [ -z "$RULE_EXISTS_SG" ]
          then
            unset RULE_EXISTS_SG
            echo "Adding rule for TCP 20048"
            aws ec2 authorize-security-group-ingress --group-id $GROUP --protocol tcp --port 20048 --region $AWS_REGION --source-group $GROUP
          else
            unset RULE_EXISTS_SG
            echo "Port TCP 20048 already allowed in security group $AWS_SG for $AWS_INSTANCE."
          fi

           # Check to see if security group already allows TCP 111
          RULE_EXISTS_SG=$(aws ec2 describe-security-groups --region $AWS_REGION --filters Name=ip-permission.from-port,Values=111 Name=ip-permission.to-port,Values=111 Name=ip-permission.group-id,Values=$AWS_SG  --output text)
          if [ -z "$RULE_EXISTS_SG" ]
          then
            unset RULE_EXISTS_SG
            echo "Adding rule for TCP 111"
            aws ec2 authorize-security-group-ingress --group-id $GROUP --protocol tcp --port 111 --region $AWS_REGION --source-group $GROUP
          else
            unset RULE_EXISTS_SG
            echo "Port TCP 111 already allowed in security group $AWS_SG for $AWS_INSTANCE."
          fi

           # Check to see if security group already allows UDP 17002
          RULE_EXISTS_SG=$(aws ec2 describe-security-groups --region $AWS_REGION --filters Name=ip-permission.from-port,Values=17002 Name=ip-permission.to-port,Values=111 Name=ip-permission.group-id,Values=$AWS_SG  --output text)
          if [ -z "$RULE_EXISTS_SG" ]
          then
            unset RULE_EXISTS_SG
            echo "Adding rule for UDP 17002"
            aws ec2 authorize-security-group-ingress --group-id $GROUP --protocol udp --port 17002 --region $AWS_REGION --source-group $GROUP
          else
            unset RULE_EXISTS_SG
            echo "Port UDP 17002 already allowed in security group $AWS_SG for $AWS_INSTANCE."
          fi

           # Check to see if security group already allows TCP 2049
          RULE_EXISTS_SG=$(aws ec2 describe-security-groups --region $AWS_REGION --filters Name=ip-permission.from-port,Values=2049 Name=ip-permission.to-port,Values=2049 Name=ip-permission.group-id,Values=$AWS_SG  --output text)
          if [ -z "$RULE_EXISTS_SG" ]
          then
            unset RULE_EXISTS_SG
            echo "Adding rule for TCP 2049"
            aws ec2 authorize-security-group-ingress --group-id $GROUP --protocol tcp --port 2049 --region $AWS_REGION --source-group $GROUP
          else
            unset RULE_EXISTS_SG
            echo "Port UDP 2049 already allowed in security group $AWS_SG for $AWS_INSTANCE."
          fi

        else
          echo "Could not find security group created by OpenShift installer, exiting."
          exit 1
        fi
      echo ""
      done

      # Get the IAM instance profile for the next step
      AWS_IAM_IP=$(aws ec2 describe-instances --instance-id $AWS_INSTANCE --region $AWS_REGION --query "Reservations[].Instances[].IamInstanceProfile[].Arn" --output text | sed 's|.*/||')
    done

    # Create the JSON for the necessary IAM permissions for Cloud Drives
    cat << EOF >> /pattern-home/credentials/px-clouddrives.json
    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Sid": "",
                "Effect": "Allow",
                "Action": [
                  "ec2:AttachVolume",
                  "ec2:ModifyVolume",
                  "ec2:DetachVolume",
                  "ec2:CreateTags",
                  "ec2:CreateVolume",
                  "ec2:DeleteTags",
                  "ec2:DeleteVolume",
                  "ec2:DescribeTags",
                  "ec2:DescribeVolumeAttribute",
                  "ec2:DescribeVolumesModifications",
                  "ec2:DescribeVolumeStatus",
                  "ec2:DescribeVolumes",
                  "ec2:DescribeInstances",
                  "autoscaling:DescribeAutoScalingGroups"
                ],
                "Resource": [
                  "*"
                ]
            }
        ]
    }
    EOF

    # Get the IAM role being used
    AWS_IAM_ROLE=$(aws iam get-instance-profile --instance-profile-name $AWS_IAM_IP --region $AWS_REGION --query "InstanceProfile.Roles[].RoleName" --output text)

    echo "Creating inline policy within IAM role $AWS_IAM_ROLE for Portworx CloudDrive permissions."

    # Attach the cloud drive permission policy to the IAM role
    aws iam put-role-policy --region $AWS_REGION --role-name $AWS_IAM_ROLE --policy-name Portworx-CloudDrive --policy-document file:///pattern-home/credentials/px-clouddrives.json
    rm /pattern-home/credentials/px-clouddrives.json

    echo "Portworx pre-reqs for AWS complete."
---
# Source: portworx/templates/storageclass/portworx-rwx.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "4"
  name: px-csi-db-shared
parameters:
  io_profile: db_remote
  repl: "3"
  sharedv4: "true"
  sharedv4_svc_type: "ClusterIP"
provisioner: pxd.portworx.com
reclaimPolicy: Delete
volumeBindingMode: Immediate
allowVolumeExpansion: true
---
# Source: portworx/templates/aws-prereqs/rbac/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: portworx-prereq-clusterrole
  annotations:
    argocd.argoproj.io/hook: PreSync
    argocd.argoproj.io/sync-wave: "-15"
rules:
  - apiGroups: ['']
    resources: ['secrets','nodes','services']
    verbs: ['get', 'list']
---
# Source: portworx/templates/storageclass/rbac/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    argocd.argoproj.io/hook: PreSync
    argocd.argoproj.io/sync-wave: "-15"
  name: portworx-sc-clusterrole
rules:
- apiGroups: ["*"]
  resources: ['pods','storageclusters']
  verbs: ['get','list']
---
# Source: portworx/templates/aws-prereqs/rbac/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: portworx-prereq-clusterrolebinding
  annotations:
    argocd.argoproj.io/hook: PreSync
    argocd.argoproj.io/sync-wave: "-15"
subjects:
- kind: ServiceAccount
  name: portworx-prereq-sa
  namespace: portworx
  apiGroup: ""
roleRef:
  kind: ClusterRole
  name: portworx-prereq-clusterrole
  apiGroup: rbac.authorization.k8s.io
---
# Source: portworx/templates/storageclass/rbac/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: portworx-sc-clusterrolebinding
  annotations:
    argocd.argoproj.io/hook: PreSync
    argocd.argoproj.io/sync-wave: "-15"
subjects:
- kind: ServiceAccount
  name: portworx-sc-sa
  namespace: portworx
  apiGroup: ""
roleRef:
  kind: ClusterRole
  name: portworx-sc-clusterrole
  apiGroup: rbac.authorization.k8s.io
---
# Source: portworx/templates/storageclass/rbac/role-ns.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  annotations:
    argocd.argoproj.io/hook: PreSync
    argocd.argoproj.io/sync-wave: "-15"
  namespace: portworx 
  name: portworx-sc-ns-role
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["*"]
---
# Source: portworx/templates/storageclass/rbac/rolebinding-ns.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: portworx-sc-ns-rolebinding
  annotations:
    argocd.argoproj.io/hook: PreSync
    argocd.argoproj.io/sync-wave: "-15"
subjects:
- kind: ServiceAccount
  name: portworx-sc-sa
  namespace: portworx
  apiGroup: ""
roleRef:
  kind: Role
  name: portworx-sc-ns-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: portworx/templates/aws-prereqs/px-aws-prereqs-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "2"
  name: px-aws-prereqs
  namespace: portworx
spec:
  parallelism: 1
  completions: 1
  activeDeadlineSeconds: 120
  backoffLimit: 1
  template:
    spec:
      serviceAccountName: portworx-prereq-sa
      containers:
        - name: px-aws-prereqs-utility
          image: quay.io/hybridcloudpatterns/utility-container
          command: ['sh', '-c', 'sh /pattern-home/scripts/px_aws_pre-reqs.sh']
          volumeMounts:
            - name: pattern-home-prereqs
              mountPath: "/pattern-home/scripts"
            - name: credentials
              mountPath: "/pattern-home/credentials"
      volumes:
        - name: pattern-home-prereqs
          configMap:
            name: px-aws-prereqs-configmap
            defaultMode: 0755
        - name: credentials
          emptyDir:
            sizeLimit: 1Mi
      restartPolicy: Never
---
# Source: portworx/templates/storageclass/wait-for-pxe.yaml
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "3"
  name: job-wait-for-portworx
  namespace: portworx
spec:
  template:
    spec:
      containers:
      - image: quay.io/hybridcloudpatterns/utility-container
        command:
        - /bin/bash
        - -x
        - -c
        - |
          stc_status=$(oc get stc -n portworx | grep -E "Online|Running" | wc -l)
          until [ "$stc_status" -eq "1" ];
          do
            echo "Portworx storagecluster not yet online"
            sleep 10
            stc_status=$(oc get stc -n portworx | grep -E "Online|Running" | wc -l)
          done
          echo "Portworx storagecluster online, waiting for all containers to start"
          num_px_pods=$(oc get pod -l name=portworx -n portworx --no-headers | wc -l)
          while [ 1 ];
          do
            num_px_pods_ready=$(oc get pod -l name=portworx -n portworx |grep -P '\s+([1-9]+[\d]*)\/\1\s+' | wc -l)
            if [ "$num_px_pods_ready" -eq "$num_px_pods" ]; then
              echo "Portworx is ready, $num_px_pods_ready of $num_px_pods pods running 2/2"
              exit 0
            fi
            echo "Portworx is not yet ready, $num_px_pods_ready of $num_px_pods pods running 2/2"
            sleep 15
          done
        name: wait-for-portworx-ready
      dnsPolicy: ClusterFirst
      restartPolicy: Never
      serviceAccount: portworx-sc-sa
      serviceAccountName: portworx-sc-sa
      terminationGracePeriodSeconds: 600
---
# Source: portworx/templates/portworx-storagecluster.yaml
apiVersion: core.libopenstorage.org/v1
kind: StorageCluster
metadata:
  name: px-cluster-region
  namespace: portworx
  annotations:
    argocd.argoproj.io/sync-wave: "3"
    portworx.io/is-eks: "true"
    portworx.io/is-openshift: "true"
    portworx.com/install-source: helm-rhmcgo
    portworx.com/helm-vars: chart="portworx-0.0.1",cloudProvider="map[storageClass:default-rwo]" ,clusterGroup="map[applications:[map[name:stormshift path:charts/factory/manuela-stormshift plugin:map[name:helm-with-kustomize] project:factory] map[name:odh namespace:manuela-factory-ml-workspace path:charts/datacenter/opendatahub project:factory]] imperative:map[jobs:[map[name:test playbook:ansible/test.yml]]] isHubCluster:false name:factory namespaces:[manuela-stormshift-line-dashboard manuela-stormshift-machine-sensor manuela-stormshift-messaging manuela-factory-ml-workspace] operatorgroupExcludes:[manuela-factory-ml-workspace] projects:[factory] subscriptions:[map[channel:stable name:opendatahub-operator source:community-operators] map[channel:stable name:seldon-operator namespace:manuela-stormshift-messaging source:community-operators] map[channel:stable name:amq-streams namespace:manuela-stormshift-messaging] map[channel:7.x name:amq-broker-rhel8 namespace:manuela-stormshift-messaging] map[channel:stable name:red-hat-camel-k namespace:manuela-stormshift-messaging]]]" ,csi="true" ,deleteStrategy="UninstallAndWipe" ,envVars="none" ,global="map[clusterDomain:region.example.com clusterPlatform:AWS hub:map[provider:aws storageClassName:gp2] hubClusterDomain:apps.hub.example.com localClusterDomain:apps.region.example.com namespace:pattern-namespace options:map[installPlanApproval:Automatic syncPolicy:Automatic useCSV:false] pattern:mypattern repoURL:https://github.com/pattern-clone/mypattern]" ,internalKVDB="true" ,main="map[clusterGroupName:hub git:map[repoURL:https://github.com/pattern-clone/mypattern revision:main]]" ,namespace="portworx" ,network="map[dataInterface:none managementInterface:none]" ,pxnamespace="portworx" ,repo="map[dr:docker.io/portworx enterprise:docker.io/portworx]" ,secretType="k8s" ,storage="map[drives:type=gp2,size=20 journalDevice:<nil> kvdbDrives:type=gp2,size=150 maxStorageNodesPerZone:1 usedrivesAndPartitions:false usefileSystemDrive:false]" ,versions="map[autoPilot:1.3.7 enterprise:2.13.4 ociMon:2.13.4 stork:23.4.0]" 
spec:
  deleteStrategy:
    type: UninstallAndWipe
  env:
    # TODO: Change this hardcoded image path to an ECR registry path with px-enterprise image (PWX-27961)
    - name: PX_IMAGE
      value: docker.io/portworx/px-enterprise:2.13.0
    - name: PX_NAMESPACE
      value: portworx
  image: "portworx/oci-monitor:2.13.4"
  imagePullPolicy: Always
  kvdb:
    internal: true
  cloudStorage:
    deviceSpecs:
    - type=gp2,size=20
    journalDeviceSpec: auto
    kvdbDeviceSpec: type=gp2,size=150
    maxStorageNodesPerZone: 1
  secretsProvider: k8s
  stork:
    enabled: true
    args:
      webhook-controller: "true"
    image: "openstorage/stork:23.4.0"
  autopilot:
    enabled: true
    image: "portworx/autopilot:1.3.7"
  csi:
    enabled: true
